{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6749829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.52\n"
     ]
    }
   ],
   "source": [
    "# 这是一个简单的单层神经网络示例\n",
    "# 包含:\n",
    "# - 3个输入神经元(a1,a2,a3)\n",
    "# - 1个输出神经元\n",
    "# - 3个连接权重(w1,w2,w3)\n",
    "# - 1个偏置项(b1)\n",
    "# 这种结构也被称为感知器(Perceptron)\n",
    "\n",
    "import numpy as np\n",
    "# 输入层\n",
    "a1=0.9\n",
    "a2=0.5\n",
    "a3=0.7\n",
    "\n",
    "# 权重\n",
    "w1=0.8\n",
    "w2=-0.4\n",
    "w3=0\n",
    "\n",
    "# 偏置\n",
    "b1=1\n",
    "\n",
    "# 计算输出\n",
    "sum1=a1*w1+a2*w2+a3*w3+b1\n",
    "print(sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ca22d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.52\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# 对神经元的输出进行激活函数处理\n",
    "# 这里使用ReLU(Rectified Linear Unit)激活函数\n",
    "# ReLU函数的特点是:\n",
    "# - 当输入x>0时,输出等于输入\n",
    "# - 当输入x≤0时,输出为0\n",
    "# 这种非线性的激活函数可以让神经网络学习更复杂的模式\n",
    "\n",
    "#激活函数\n",
    "def activation_ReLU(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "print(activation_ReLU(sum1))\n",
    "print(activation_ReLU(-sum1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cf31f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.52]\n"
     ]
    }
   ],
   "source": [
    "# 使用numpy的向量化运算重写神经网络计算\n",
    "# np.array()创建输入和权重的向量\n",
    "# np.dot()计算向量点积,相当于加权和\n",
    "# 这种向量化的写法比单独计算每个神经元更高效\n",
    "\n",
    "# 在numpy中,一维数组默认被视为行向量\n",
    "# 例如 np.array([1,2,3]) 是一个1x3的行向量\n",
    "# np.dot(a,b)计算的是:a的行与b的列的点积\n",
    "\n",
    "inputs=np.array([a1,a2,a3])\n",
    "\n",
    "weights=np.array([[w1],[w2],[w3]])\n",
    "\n",
    "bias=1\n",
    "\n",
    "output=np.dot(inputs,weights)+bias\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f721d394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.020000000000000018\n"
     ]
    }
   ],
   "source": [
    "# 下面我们将构建一个具有两个神经元的神经网络\n",
    "# 每个神经元都有3个输入(a1,a2,a3)\n",
    "# 每个输入都有对应的权重(w11,w21,w31和w12,w22,w32)\n",
    "# 每个神经元都有一个偏置值(b11,b21)\n",
    "# 这样的结构允许网络学习更复杂的特征\n",
    "\n",
    "# 输入层\n",
    "a1=-0.9\n",
    "a2=-0.5\n",
    "a3=-0.7\n",
    "\n",
    "# 权重\n",
    "w11=0.8\n",
    "w21=-0.4\n",
    "w31=0\n",
    "\n",
    "w12=0.7\n",
    "w22=-0.6\n",
    "w32=0.2\n",
    "\n",
    "b11=0.5\n",
    "b21=0.5\n",
    "\n",
    "sum1=a1*w11+a2*w21+a3*w31+b11\n",
    "sum2=a1*w12+a2*w22+a3*w32+b21\n",
    "\n",
    "print(sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7297081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02  0.03]\n",
      "[0.   0.03]\n"
     ]
    }
   ],
   "source": [
    "# 权重矩阵\n",
    "# inputs 是 1x3 的行向量\n",
    "# weights 应该是 3x2 的矩阵,每列对应一个神经元的权重\n",
    "\n",
    "inputs = np.array([a1,a2,a3])  # 1x3\n",
    "\n",
    "weights = np.array([[w11,w12],  # 3x2\n",
    "                    [w21,w22],\n",
    "                    [w31,w32]])\n",
    "\n",
    "bias = np.array([b11,b21])  # 1x2\n",
    "\n",
    "sum = np.dot(inputs,weights) + bias  # (1x3)x(3x2) + (1x2) = 1x2\n",
    "\n",
    "print(sum)\n",
    "print(activation_ReLU(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75c39d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06 -0.03]\n",
      " [ 0.06  0.12]\n",
      " [ 0.42  0.59]]\n",
      "--------------------------------\n",
      "[[0.   0.  ]\n",
      " [0.06 0.12]\n",
      " [0.42 0.59]]\n"
     ]
    }
   ],
   "source": [
    "# 在这里我们将处理多个样本(batch)的情况\n",
    "# inputs 变成了一个 3x3 的矩阵,每行代表一个样本\n",
    "# weights 保持 3x2 的形状不变\n",
    "# bias 仍然是 1x2,但会自动广播到每个样本\n",
    "# 最终输出是一个 3x2 的矩阵,每行代表一个样本的两个神经元输出\n",
    "\n",
    "# 输入层\n",
    "a11=-0.9\n",
    "a21=-0.4\n",
    "a31=-0.7\n",
    "\n",
    "a12=-0.8\n",
    "a22=-0.5\n",
    "a32=-0.6\n",
    "\n",
    "a13=-0.5\n",
    "a23=-0.8\n",
    "a33=-0.2\n",
    "\n",
    "#batch_size=3\n",
    "inputs = np.array([[a11,a21,a31],\n",
    "                    [a12,a22,a32],\n",
    "                    [a13,a23,a33],]) \n",
    "\n",
    "weights = np.array([[w11,w12],  # 3x2\n",
    "                    [w21,w22],\n",
    "                    [w31,w32]])\n",
    "\n",
    "bias = np.array([b11,b21])  # 1x2\n",
    "\n",
    "sum = np.dot(inputs,weights) + bias  \n",
    "\n",
    "print(sum)\n",
    "print(\"--------------------------------\")\n",
    "print(activation_ReLU(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca2b0a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.22658719e-02  1.25637693e-02]\n",
      " [-3.28494740e-01  9.17016345e-04]\n",
      " [-1.00937677e+00  1.66992737e-01]]\n"
     ]
    }
   ],
   "source": [
    "#人工函数自动生成权重矩阵\n",
    "#数据量太大的时候需要自动生成权重矩阵\n",
    "def create_weights(input_size,output_size):\n",
    "    return np.random.randn(input_size,output_size)\n",
    "\n",
    "print(create_weights(3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2898c073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.53425091 -0.40318797]\n"
     ]
    }
   ],
   "source": [
    "# 对于偏置(bias),我们只需要神经元的数量,因为:\n",
    "# 1. 每个神经元只需要一个偏置值\n",
    "# 2. 这个偏置值会被自动广播到所有输入样本\n",
    "# 3. 不同于权重矩阵需要input_size x output_size的形状,\n",
    "#    偏置只需要与输出层神经元数量相同的一维向量即可\n",
    "\n",
    "def create_biases(n_neurons): \n",
    "    return np.random.randn(n_neurons)\n",
    "\n",
    "biases=create_biases(2)\n",
    "print(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57465f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.9 -0.4 -0.7]\n",
      " [-0.8 -0.5 -0.6]\n",
      " [-0.5 -0.8 -0.2]]\n",
      "[[ 0.8  0.7]\n",
      " [-0.4 -0.6]\n",
      " [ 0.   0.2]]\n",
      "[-1.53425091 -0.40318797]\n",
      "[[-2.09425091 -0.93318797]\n",
      " [-1.97425091 -0.78318797]\n",
      " [-1.61425091 -0.31318797]]\n"
     ]
    }
   ],
   "source": [
    "print(inputs)\n",
    "print(weights)\n",
    "print(biases)\n",
    "print(np.dot(inputs,weights)+biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e40e79",
   "metadata": {},
   "source": [
    "| 含义     | 数学维度                  | 说明                   |\n",
    "| ------ | --------------------- | -------------------- |\n",
    "| 输入 $X$ | $m \\times n$          | $m$ 个样本，每个 $n$ 维特征   |\n",
    "| 权重 $W$ | $n \\times p$          | $p$ 个神经元，每个连 $n$ 个输入 |\n",
    "| 输出 $Z$ | $m \\times p$          | 每个样本对应 $p$ 个输出       |\n",
    "| 偏置 $b$ | $1 \\times p$ 或 $(p,)$ | 每个神经元一个偏置，广播到每个样本    |\n",
    "| 广播结果   | $m \\times p$          | 和输出相同，元素级加法          |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
